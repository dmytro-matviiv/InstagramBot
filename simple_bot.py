#!/usr/bin/env python3
"""
–°–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è Instagram –±–æ—Ç–∞ –±–µ–∑ –ø–æ—à—É–∫—É –∑–æ–±—Ä–∞–∂–µ–Ω—å
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∑–∞–≥–ª—É—à–∫—É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∞–±–æ –±–µ—Ä–µ—Ç—å—Å—è –∑ –Ω–æ–≤–∏–Ω–∏
"""

import os
import time
import random
import logging
from datetime import datetime
from news_collector import NewsCollector
from content_generator import ContentGenerator
from instagram_publisher import InstagramPublisher
from translator import NewsTranslator
from PIL import Image
import requests
from io import BytesIO
from bs4 import BeautifulSoup

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è –±–µ–∑ –µ–º–æ–¥–∑—ñ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('instagram_bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class SimpleInstagramBot:
    def __init__(self):
        self.news_collector = NewsCollector()
        self.content_generator = ContentGenerator()
        self.instagram_publisher = InstagramPublisher()
        self.translator = NewsTranslator()
        self.posted_articles = self.load_posted_articles()
    
    def load_posted_articles(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å–ø–∏—Å–æ–∫ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π –∑ —Ñ–∞–π–ª—É"""
        try:
            import json
            with open('posted_articles.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data)
        except (FileNotFoundError, json.JSONDecodeError):
            return set()
    
    def save_posted_articles(self):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Å–ø–∏—Å–æ–∫ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π —É —Ñ–∞–π–ª"""
        try:
            import json
            with open('posted_articles.json', 'w', encoding='utf-8') as f:
                json.dump(list(self.posted_articles), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–ø–∏—Å–∫—É –ø–æ—Å—Ç—ñ–≤: {e}")


    

    
    def extract_images_from_html(self, html_content):
        """–í–∏—Ç—è–≥—É—î URL –∑–æ–±—Ä–∞–∂–µ–Ω—å –∑ HTML –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
        import re
        img_urls = []
        
        # –®—É–∫–∞—î–º–æ img —Ç–µ–≥–∏
        img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
        matches = re.findall(img_pattern, html_content, re.IGNORECASE)
        
        for url in matches:
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –æ—á–µ–≤–∏–¥–Ω–æ –Ω–µ–ø—Ä–∏–¥–∞—Ç–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            if any(skip in url.lower() for skip in ['icon', 'logo', 'avatar', 'button', '1x1', 'pixel']):
                continue
            # –î–æ–¥–∞—î–º–æ —è–∫—â–æ —Ü–µ —Å—Ö–æ–∂–µ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                img_urls.append(url)
        
        return img_urls
    
    def try_get_larger_image_url(self, url):
        """–ù–∞–º–∞–≥–∞—î—Ç—å—Å—è –∑–Ω–∞–π—Ç–∏ –±—ñ–ª—å—à—É –≤–µ—Ä—Å—ñ—é –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è"""
        if not url:
            return url
            
        # –®–∞–±–ª–æ–Ω–∏ –¥–ª—è –∑–∞–º—ñ–Ω–∏ –º—ñ–Ω—ñ–∞—Ç—é—Ä –Ω–∞ –ø–æ–≤–Ω–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        replacements = [
            # –ó–∞–≥–∞–ª—å–Ω—ñ —à–∞–±–ª–æ–Ω–∏
            ('thumb_', 'full_'),
            ('small_', 'large_'),
            ('mini_', 'full_'),
            ('/thumbs/', '/images/'),
            ('/small/', '/large/'),
            ('_150.', '_1200.'),
            ('_200.', '_1200.'),
            ('_300.', '_1200.'),
            ('_400.', '_1200.'),
            ('_500.', '_1200.'),
            ('_thumb.', '_full.'),
            ('_small.', '_large.'),
            # –†–æ–∑–º—ñ—Ä–∏ –≤ URL
            ('150x', '1200x'),
            ('200x', '1200x'),
            ('300x', '1200x'),
            ('400x', '1200x'),
            ('630x360', '1200x800'),
            ('320x240', '1280x960'),
        ]
        
        original_url = url
        for old_pattern, new_pattern in replacements:
            if old_pattern in url.lower():
                new_url = url.replace(old_pattern, new_pattern)
                if new_url != url:
                    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ –±—ñ–ª—å—à–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {old_pattern} -> {new_pattern}")
                    return new_url
        
        return original_url
    
    def detect_news_category(self, title, content):
        """–í–∏–∑–Ω–∞—á–∞—î –∫–∞—Ç–µ–≥–æ—Ä—ñ—é –Ω–æ–≤–∏–Ω–∏ –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
        text = f"{title} {content}".lower()
        
        # –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è –≤—ñ–π—Å—å–∫–æ–≤–∏—Ö –Ω–æ–≤–∏–Ω
        war_keywords = [
            '–≤—ñ–π–Ω–∞', 'war', '—Ñ—Ä–æ–Ω—Ç', 'front', '–±–æ–π–æ–≤—ñ', 'combat', '–Ω–∞—Å—Ç—É–ø', 'offensive',
            '–æ–±–æ—Ä–æ–Ω–∞', 'defense', '–æ–±—Å—Ç—Ä—ñ–ª', 'shelling', '—Ä–∞–∫–µ—Ç', 'missile', '–¥—Ä–æ–Ω', 'drone',
            '–≤—ñ–π—Å—å–∫–æ–≤', 'military', '–∞—Ä–º—ñ—è', 'army', '–≤—Ç—Ä–∞—Ç–∏', 'casualties', '–∑–∞–≥–∏–±–ª—ñ',
            '–ø–æ—Ä–∞–Ω–µ–Ω—ñ', 'wounded', '—Ç–∞–Ω–∫', 'tank', '–∞—Ä—Ç–∏–ª–µ—Ä—ñ—è', 'artillery',
            '–∞–≤—ñ–∞—É–¥–∞—Ä', 'airstrike', '–æ–∫—É–ø–∞—Ü', 'occupation', '–∑–≤—ñ–ª—å–Ω–µ–Ω', 'liberation'
        ]
        
        # –ü–æ–ª—ñ—Ç–∏—á–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
        politics_keywords = [
            '–ø–æ–ª—ñ—Ç–∏–∫–∞', 'politics', '—É—Ä—è–¥', 'government', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', 'president',
            '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç', 'parliament', '–º—ñ–Ω—ñ—Å—Ç—Ä', 'minister', '–∑–∞–∫–æ–Ω', 'law',
            '—Ä—ñ—à–µ–Ω–Ω—è', 'decision', '—Å–∞–Ω–∫—Ü—ñ—ó', 'sanctions'
        ]
        
        # –ú—ñ–∂–Ω–∞—Ä–æ–¥–Ω—ñ –Ω–æ–≤–∏–Ω–∏
        world_keywords = [
            '–Ω–∞—Ç–æ', 'nato', '—î–≤—Ä–æ—Å–æ—é–∑', 'eu', '—Å—à–∞', 'usa', '—Ä–æ—Å—ñ—è', 'russia',
            '–º—ñ–∂–Ω–∞—Ä–æ–¥–Ω', 'international', '–¥–∏–ø–ª–æ–º–∞—Ç', 'diplomatic'
        ]
        
        if any(keyword in text for keyword in war_keywords):
            return 'war'
        elif any(keyword in text for keyword in politics_keywords):
            return 'politics'
        elif any(keyword in text for keyword in world_keywords):
            return 'world'
        else:
            return 'news'
    
    def find_news_with_image(self):
        """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –ø–µ—Ä—à—É –Ω–æ–≤–∏–Ω—É –∑ —è–∫—ñ—Å–Ω–∏–º –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º"""
        logging.info("üîç –ü–æ—à—É–∫ –Ω–æ–≤–∏–Ω–∏ –∑ —è–∫—ñ—Å–Ω–∏–º —Ñ–æ—Ç–æ...")
        
        all_news = self.news_collector.collect_fresh_news()
        if not all_news:
            logging.error("‚ùå RSS –¥–∂–µ—Ä–µ–ª–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ")
            return None
            
        analyzed_count = 0
        
        # –®—É–∫–∞—î–º–æ –ø–µ—Ä—à—É –Ω–æ–≤–∏–Ω—É –∑ —Ñ–æ—Ç–æ
        for article in all_news:
            analyzed_count += 1
            
            # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≤–∂–µ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω—ñ
            article_id = hash(article.get('title', '') + article.get('link', ''))
            if article_id in self.posted_articles:
                continue
                
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —è–∫—ñ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title = article.get('title', '')
            if not title or len(title) < 10 or title == '–ù–æ–≤–∏–Ω–∞ –∑ RSS':
                continue
                
            logging.info(f"üìä –ü–µ—Ä–µ–≤—ñ—Ä—è—é #{analyzed_count}: {title[:50]}...")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î —è–∫—ñ—Å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            image = self.get_image_from_news(article)
            
            if image:
                logging.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–∏–Ω—É –∑ —Ñ–æ—Ç–æ: {title[:50]}...")
                return {
                    'article': article,
                    'image': image
                }
            
            # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–µ—Ä–µ–≤—ñ—Ä–æ–∫
            if analyzed_count >= 50:
                break
                
        logging.warning("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–∏–Ω –∑ —è–∫—ñ—Å–Ω–∏–º–∏ —Ñ–æ—Ç–æ")
        return None

    def create_and_publish_post(self, max_attempts=50):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—è –ø–æ—Å—Ç–∞"""
        try:
            logging.info("üéØ –ü–æ—à—É–∫ –Ω–æ–≤–∏–Ω–∏ –∑ —è–∫—ñ—Å–Ω–∏–º —Ñ–æ—Ç–æ...")
            
            if max_attempts <= 0:
                logging.error("‚ùå –î–æ—Å—è–≥–Ω—É—Ç–æ –º–∞–∫—Å–∏–º—É–º —Å–ø—Ä–æ–±")
                return False
            
            # 1. –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–æ–≤–∏–Ω—É –∑ —Ñ–æ—Ç–æ
            news_data = self.find_news_with_image()
            if not news_data:
                logging.error("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–∏–Ω –∑ —è–∫—ñ—Å–Ω–∏–º–∏ —Ñ–æ—Ç–æ")
                return False
                
            # –í–∏—Ç—è–≥—É—î–º–æ –¥–∞–Ω—ñ
            news_article = news_data['article']
            image = news_data['image']
            
            logging.info(f"‚úÖ –û–±—Ä–∞–Ω–æ –Ω–æ–≤–∏–Ω—É: {news_article.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')}")
            logging.info(f"üì∏ –†–æ–∑–º—ñ—Ä —Ñ–æ—Ç–æ: {image.size[0]}x{image.size[1]}")
            
            # –î–æ–¥–∞—î–º–æ –¥–æ —Å–ø–∏—Å–∫—É –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–∏—Ö
            article_id = hash(news_article.get('title', '') + news_article.get('link', ''))
            self.posted_articles.add(article_id)
            self.save_posted_articles()  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–æ–¥–∞–≤–∞–Ω–Ω—è
            
            # –ü–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ –Ω–æ–≤–∏–Ω—É –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É
            logging.info("–ü–µ—Ä–µ–∫–ª–∞–¥ –Ω–æ–≤–∏–Ω–∏ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É...")
            ukrainian_article = self.translator.translate_news_article(news_article)
            logging.info(f"–ü–µ—Ä–µ–∫–ª–∞–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {ukrainian_article.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')}")
            
            # 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ë–ï–ó –æ–±—Ä–æ–±–∫–∏
            logging.info("‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ —Ñ–æ—Ç–æ –±–µ–∑ –æ–±—Ä–æ–±–∫–∏")
            processed_img = image
                
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            image_filename = f"post_{timestamp}.jpg"
            image_path = f"temp_images/{image_filename}"
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–∞–ø–∫—É —è–∫—â–æ —ó—ó –Ω–µ–º–∞—î
            os.makedirs("temp_images", exist_ok=True)
            
            processed_img.save(image_path, "JPEG", quality=95)
            logging.info(f"‚úÖ –†–ï–ê–õ–¨–ù–ï —Ñ–æ—Ç–æ –∑ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {image_path}")
            
            # 3. –ì–µ–Ω–µ—Ä—É—î–º–æ –∫–æ–Ω—Ç–µ–Ω—Ç –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó —Å—Ç–∞—Ç—Ç—ñ
            logging.info("–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–Ω—Ç—É...")
            post_content = self.content_generator.create_full_post(
                ukrainian_article.get('title', ''),
                ukrainian_article.get('text', ukrainian_article.get('summary', ''))
            )
            
            logging.info(f"–ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ –ø–æ—Å—Ç ({len(post_content)} —Å–∏–º–≤–æ–ª—ñ–≤)")
            
            # 4. –ü—É–±–ª—ñ–∫—É—î–º–æ
            logging.info("–ü—É–±–ª—ñ–∫–∞—Ü—ñ—è –≤ Instagram...")
            success, message = self.instagram_publisher.safe_publish(
                image_path, 
                post_content,
                add_hashtags_as_comment=True
            )
            
            if success:
                logging.info(f"–ü–æ—Å—Ç —É—Å–ø—ñ—à–Ω–æ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ! {message}")
                self.posted_articles.add(article_id)
                return True
            else:
                logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó: {message}")
                return False
            
        except Exception as e:
            logging.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
            return False
    
    def test_run(self):
        """–¢–µ—Å—Ç –≤—Å—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤"""
        logging.info("–¢–µ—Å—Ç–æ–≤–∏–π —Ä–µ–∂–∏–º...")
        
        # –¢–µ—Å—Ç –Ω–æ–≤–∏–Ω –∑ fallback –º–µ—Ö–∞–Ω—ñ–∑–º–æ–º
        logging.info("–¢–µ—Å—Ç –∑–±–æ—Ä—É –Ω–æ–≤–∏–Ω...")
        news = self.news_collector.get_random_news()
        if news:
            logging.info(f"–ù–æ–≤–∏–Ω–∏ –ø—Ä–∞—Ü—é—é—Ç—å: {news.get('title', 'N/A')[:50]}...")
        else:
            logging.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑—ñ–±—Ä–∞—Ç–∏ –Ω–æ–≤–∏–Ω–∏ –∑ RSS, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é —Ç–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ...")
            # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ —Ä–æ–±–æ—Ç—É –∑ —Ç–µ—Å—Ç–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏
            logging.info("–ù–æ–≤–∏–Ω–∏ –ø—Ä–∞—Ü—é—é—Ç—å: –¢–µ—Å—Ç–æ–≤–∞ –Ω–æ–≤–∏–Ω–∞ (fallback —Ä–µ–∂–∏–º)...")
        
        # –¢–µ—Å—Ç –¢–Ü–õ–¨–ö–ò —Ä–µ–∞–ª—å–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
        logging.info("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é —Ç–µ—Å—Ç —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è - –±–æ—Ç –ø—Ä–∞—Ü—é—î –¢–Ü–õ–¨–ö–ò –∑ —Ä–µ–∞–ª—å–Ω–∏–º–∏ —Ñ–æ—Ç–æ –∑ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É")
        logging.info("‚úÖ –¢–µ—Å—Ç –∑–æ–±—Ä–∞–∂–µ–Ω—å –ø—Ä–æ–ø—É—â–µ–Ω–æ - —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ñ —Ñ–æ—Ç–æ")
        
        # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç—É
        logging.info("–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç—É...")
        content = self.content_generator.create_full_post(
            "–¢–µ—Å—Ç–æ–≤–∞ –Ω–æ–≤–∏–Ω–∞", "–¶–µ —Ç–µ—Å—Ç–æ–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç"
        )
        logging.info(f"–ö–æ–Ω—Ç–µ–Ω—Ç –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ: {len(content)} —Å–∏–º–≤–æ–ª—ñ–≤")
        
        # –¢–µ—Å—Ç Instagram (–ë–ï–ó —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–ª—è –±–µ–∑–ø–µ–∫–∏)
        logging.info("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é —Ç–µ—Å—Ç Instagram –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –±–ª–æ–∫—É–≤–∞–Ω—å")
        logging.info("‚úÖ Instagram —Ç–µ—Å—Ç –ø—Ä–æ–ø—É—â–µ–Ω–æ –¥–ª—è –±–µ–∑–ø–µ–∫–∏")
        
        logging.info("–í—Å—ñ —Ç–µ—Å—Ç–∏ –ø—Ä–æ–π–¥–µ–Ω–æ!")
        return True
    
    def analyze_rss_quality(self):
        """–ù–û–í–ò–ô: –ê–Ω–∞–ª—ñ–∑—É—î —è–∫—ñ—Å—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω—å —É –≤—Å—ñ—Ö RSS –¥–∂–µ—Ä–µ–ª–∞—Ö"""
        logging.info("üîç –ü–û–ß–ê–¢–û–ö –ê–ù–ê–õ–Ü–ó–£ RSS –î–ñ–ï–†–ï–õ...")
        logging.info("=" * 80)
        
        good_sources = []
        bad_sources = []
        
        for i, source in enumerate(self.news_collector.sources):
            logging.info(f"\nüì∞ –ê–Ω–∞–ª—ñ–∑—É—é –¥–∂–µ—Ä–µ–ª–æ {i+1}/{len(self.news_collector.sources)}: {source}")
            logging.info("-" * 60)
            
            try:
                articles = self.news_collector.fetch_rss_news(source)
                suitable_articles = 0
                total_articles = len(articles)
                
                if not articles:
                    logging.info("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–æ–≤–∏–Ω–∏ –∑ —Ü—å–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞")
                    bad_sources.append((source, "–ù–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π"))
                    continue
                
                for j, article in enumerate(articles[:10]):  # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–µ—Ä—à—ñ 10 –Ω–æ–≤–∏–Ω
                    title = article.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')[:50]
                    logging.info(f"\n  üìÑ –ù–æ–≤–∏–Ω–∞ {j+1}: {title}...")
                    
                    # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
                    image = self.get_image_from_news(article, test_mode=True)
                    if image:
                        suitable_articles += 1
                        logging.info(f"  ‚úÖ –ü—ñ–¥—Ö–æ–¥—è—â—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–Ω–∞–π–¥–µ–Ω–æ!")
                    else:
                        logging.info(f"  ‚ùå –ù–µ–º–∞—î –ø—ñ–¥—Ö–æ–¥—è—â–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å")
                
                # –û—Ü—ñ–Ω—é—î–º–æ —è–∫—ñ—Å—Ç—å –¥–∂–µ—Ä–µ–ª–∞
                success_rate = (suitable_articles / total_articles) * 100 if total_articles > 0 else 0
                
                if success_rate >= 5:  # –ú—ñ–Ω—ñ–º—É–º 5% –Ω–æ–≤–∏–Ω –∑ —è–∫—ñ—Å–Ω–∏–º–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º–∏ (—Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–æ)
                    logging.info(f"\n  ‚úÖ –î–ñ–ï–†–ï–õ–û –ü–Ü–î–•–û–î–ò–¢–¨: {suitable_articles}/{total_articles} –Ω–æ–≤–∏–Ω –∑ —è–∫—ñ—Å–Ω–∏–º–∏ —Ñ–æ—Ç–æ ({success_rate:.1f}%)")
                    good_sources.append((source, success_rate))
                else:
                    logging.info(f"\n  ‚ùå –î–ñ–ï–†–ï–õ–û –ù–ï –ü–Ü–î–•–û–î–ò–¢–¨: {suitable_articles}/{total_articles} –Ω–æ–≤–∏–Ω –∑ —è–∫—ñ—Å–Ω–∏–º–∏ —Ñ–æ—Ç–æ ({success_rate:.1f}%)")
                    bad_sources.append((source, f"{success_rate:.1f}% —è–∫—ñ—Å–Ω–∏—Ö —Ñ–æ—Ç–æ"))
                    
            except Exception as e:
                logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É {source}: {e}")
                bad_sources.append((source, f"–ü–æ–º–∏–ª–∫–∞: {e}"))
        
        # –ü—ñ–¥—Å—É–º–æ–∫ –∞–Ω–∞–ª—ñ–∑—É
        logging.info("\n" + "=" * 80)
        logging.info("üìä –ü–Ü–î–°–£–ú–û–ö –ê–ù–ê–õ–Ü–ó–£ RSS –î–ñ–ï–†–ï–õ")
        logging.info("=" * 80)
        
        logging.info(f"\n‚úÖ –•–û–†–û–®–Ü –î–ñ–ï–†–ï–õ–ê ({len(good_sources)}):")
        for source, rate in good_sources:
            logging.info(f"  ‚Ä¢ {source} ({rate:.1f}% —è–∫—ñ—Å–Ω–∏—Ö —Ñ–æ—Ç–æ)")
        
        logging.info(f"\n‚ùå –ü–û–ì–ê–ù–Ü –î–ñ–ï–†–ï–õ–ê ({len(bad_sources)}):")
        for source, reason in bad_sources:
            logging.info(f"  ‚Ä¢ {source} - {reason}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        if len(good_sources) < 5:
            logging.info(f"\n‚ö†Ô∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–Ø: –ü–æ—Ç—Ä—ñ–±–Ω–æ –±—ñ–ª—å—à–µ —è–∫—ñ—Å–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª (–∑–∞—Ä–∞–∑ {len(good_sources)})")
            self.suggest_new_rss_sources()
        else:
            logging.info(f"\nüéâ –í–Ü–î–ú–Ü–ù–ù–û: –ó–Ω–∞–π–¥–µ–Ω–æ {len(good_sources)} —è–∫—ñ—Å–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª!")
        
        return good_sources, bad_sources
    
    def suggest_new_rss_sources(self):
        """–ü—Ä–æ–ø–æ–Ω—É—î –Ω–æ–≤—ñ RSS –¥–∂–µ—Ä–µ–ª–∞ –∑ —è–∫—ñ—Å–Ω–∏–º–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º–∏"""
        logging.info("\nüí° –ü–†–û–ü–û–ù–û–í–ê–ù–Ü –ù–û–í–Ü RSS –î–ñ–ï–†–ï–õ–ê:")
        
        new_sources = [
            'https://www.dw.com/uk/rss/news',  # Deutsche Welle —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é
            'https://www.ukrinform.ua/rss/block-war',  # –£–∫—Ä—ñ–Ω—Ñ–æ—Ä–º –≤—ñ–π—Å—å–∫–æ–≤—ñ –Ω–æ–≤–∏–Ω–∏
            'https://espreso.tv/rss',  # –ï—Å–ø—Ä–µ—Å–æ –¢–í
            'https://www.eurointegration.com.ua/rss/',  # –Ñ–≤—Ä–æ–ø–µ–π—Å—å–∫–∞ –ø—Ä–∞–≤–¥–∞
            'https://interfax.com.ua/news/rss/',  # –Ü–Ω—Ç–µ—Ä—Ñ–∞–∫—Å –£–∫—Ä–∞—ó–Ω–∞
            'https://censor.net/includes/news_rss.php',  # –¶–µ–Ω–∑–æ—Ä.–Ω–µ—Ç
            'https://www.obozrevatel.com/rss.xml',  # –û–±–æ–∑—Ä–µ–≤–∞—Ç–µ–ª—å
            'https://gordonua.com/xml/rss_category/1.html',  # –ì–æ—Ä–¥–æ–Ω
        ]
        
        for source in new_sources:
            logging.info(f"  ‚Ä¢ {source}")
        
        logging.info("\nüîß –î–æ–¥–∞–π—Ç–µ —Ü—ñ –¥–∂–µ—Ä–µ–ª–∞ –¥–æ config.py –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ!")
    
    def get_image_from_news(self, news_article):
        """–û—Ç—Ä–∏–º—É—î –Ø–ö–Ü–°–ù–ï –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –Ω–æ–≤–∏–Ω–∏"""
        return self._analyze_images_only(news_article)
    

    def _analyze_images_only(self, news_article):
        """–¢—ñ–ª—å–∫–∏ –∞–Ω–∞–ª—ñ–∑—É—î —Ä–µ–∞–ª—å–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –±–µ–∑ fallback - –∑ –ø–æ–≤–Ω–∏–º –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è–º —Å—Ç–∞—Ç—Ç—ñ"""
        # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ –≤–∑—è—Ç–∏ —Ä–µ–∞–ª—å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –Ω–æ–≤–∏–Ω–∏
        image_urls = []
        
        # 1. –î–æ–¥–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ RSS
        if news_article.get('rss_image'):
            image_urls.append(news_article['rss_image'])
        if news_article.get('top_image'):
            image_urls.append(news_article['top_image'])
        if news_article.get('images'):
            image_urls.extend(news_article['images'][:5])
            
        # 2. –î–æ–¥–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –æ–ø–∏—Å—É RSS
        description = news_article.get('description', '') or news_article.get('summary', '')
        if description:
            img_urls_from_desc = self.extract_images_from_html(description)
            image_urls.extend(img_urls_from_desc)
        
        # 3. –ì–û–õ–û–í–ù–ï: –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø–æ–≤–Ω—É —Å—Ç–∞—Ç—Ç—é –∑ —Å–∞–π—Ç—É –¥–ª—è –∫—Ä–∞—â–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
        article_url = news_article.get('link', '')
        if article_url:
            logging.info(f"üîç –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –ø–æ–≤–Ω—É —Å—Ç–∞—Ç—Ç—é –¥–ª—è –∫—Ä–∞—â–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å: {article_url[:50]}...")
            page_images = self.extract_images_from_full_article(article_url)
            image_urls.extend(page_images)
        
        # 4. –ü—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –≤–µ—Ä—Å—ñ—ó –∑–æ–±—Ä–∞–∂–µ–Ω—å
        enhanced_urls = []
        for url in image_urls:
            enhanced_url = self.try_get_larger_image_url(url)
            if enhanced_url != url:
                enhanced_urls.append(enhanced_url)
        image_urls.extend(enhanced_urls)
            
        # –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
        image_urls = list(set([url for url in image_urls if url and url.strip()]))
        
        if not image_urls:
            return None
            
        # –Ü–º–ø–æ—Ä—Ç—É—î–º–æ –≤–∏–º–æ–≥–∏ –¥–æ —è–∫–æ—Å—Ç—ñ
        from config import IMAGE_REQUIREMENTS
        min_width = IMAGE_REQUIREMENTS['min_width']
        min_height = IMAGE_REQUIREMENTS['min_height'] 
        min_pixels = IMAGE_REQUIREMENTS['min_pixels']
        
        logging.info(f"üìè –ü–æ—Ç–æ—á–Ω—ñ –≤–∏–º–æ–≥–∏: –º—ñ–Ω. {min_width}x{min_height}, –º—ñ–Ω. –ø—ñ–∫—Å–µ–ª—ñ–≤: {min_pixels}")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–∂–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        for i, image_url in enumerate(image_urls):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                    'Accept-Language': 'uk-UA,uk;q=0.9,en;q=0.8',
                    'Referer': news_article.get('link', ''),
                    'Connection': 'keep-alive'
                }
                response = requests.get(image_url, timeout=10, headers=headers, stream=True)
                
                if response.status_code == 200:
                    content_length = response.headers.get('content-length')
                    if content_length and int(content_length) < 5000:
                        continue
                        
                    img = Image.open(BytesIO(response.content))
                    width, height = img.size
                    
                    # –†–ï–ê–õ–Ü–°–¢–ò–ß–ù–Ü –í–ò–ú–û–ì–ò –î–û –Ø–ö–û–°–¢–Ü
                    logging.info(f"üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—é –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {width}x{height}")
                    
                    if width < min_width or height < min_height:  # 600x400 –º—ñ–Ω—ñ–º—É–º
                        logging.warning(f"‚ùå –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞–º–∞–ª–µ: {width}x{height} (–ø–æ—Ç—Ä—ñ–±–Ω–æ –º—ñ–Ω—ñ–º—É–º {min_width}x{min_height})")
                        continue
                        
                    total_pixels = width * height
                    if total_pixels < min_pixels:  # 240K –ø—ñ–∫—Å–µ–ª—ñ–≤ –º—ñ–Ω—ñ–º—É–º
                        logging.warning(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø—ñ–∫—Å–µ–ª—ñ–≤: {total_pixels} (–ø–æ—Ç—Ä—ñ–±–Ω–æ –º—ñ–Ω—ñ–º—É–º {min_pixels})")
                        continue
                    
                    # –ü–†–ò–ô–ú–ê–Ñ–ú–û –í–°–Ü –û–†–Ü–Ñ–ù–¢–ê–¶–Ü–á - –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ñ, –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ñ, –∫–≤–∞–¥—Ä–∞—Ç–Ω—ñ
                    logging.info(f"‚úÖ –ó–ù–ê–ô–î–ï–ù–û –Ø–ö–Ü–°–ù–ï –§–û–¢–û: {width}x{height} ({total_pixels} –ø—ñ–∫—Å–µ–ª—ñ–≤)")
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ RGB –¥–ª—è Instagram (–≤–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫—É –∑ mode P)
                    if img.mode != 'RGB':
                        original_mode = img.mode
                        img = img.convert('RGB')
                        logging.info(f"üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–æ –∑ {original_mode} –≤ RGB")
                    
                    logging.info(f"üéâ –ü–û–í–ï–†–¢–ê–Æ –Ø–ö–Ü–°–ù–ï –ó–û–ë–†–ê–ñ–ï–ù–ù–Ø {width}x{height}")
                    # –Ø–∫—â–æ –¥—ñ–π—à–ª–∏ —Å—é–¥–∏ - –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø—ñ–¥—Ö–æ–¥–∏—Ç—å!
                    return img
                    
            except Exception:
                continue
        
        return None  # –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—ñ–¥—Ö–æ–¥—è—â–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
    
    def extract_images_from_full_article(self, article_url):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –ø–æ–≤–Ω—É —Å—Ç–∞—Ç—Ç—é —ñ –≤–∏—Ç—è–≥—É—î –í–°–Ü —è–∫—ñ—Å–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –û–†–ò–ì–Ü–ù–ê–õ–¨–ù–ò–ú–ò —Ä–æ–∑–º—ñ—Ä–∞–º–∏"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'uk-UA,uk;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            response = requests.get(article_url, headers=headers, timeout=15)
            if response.status_code != 200:
                return []
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            image_urls = []
            priority_images = []
            
            # –®—É–∫–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ —Ä—ñ–∑–Ω–∏—Ö —Ç–µ–≥–∞—Ö
            img_tags = soup.find_all('img')
            
            for img in img_tags:
                # –û—Ç—Ä–∏–º—É—î–º–æ URL –∑ —Ä—ñ–∑–Ω–∏—Ö –∞—Ç—Ä–∏–±—É—Ç—ñ–≤ - –≤–∫–ª—é—á–∞—é—á–∏ –û–†–ò–ì–Ü–ù–ê–õ–¨–ù–Ü –≤–µ—Ä—Å—ñ—ó
                potential_srcs = [
                    img.get('data-original'),    # –û—Ä–∏–≥—ñ–Ω–∞–ª
                    img.get('data-full'),        # –ü–æ–≤–Ω–∏–π —Ä–æ–∑–º—ñ—Ä
                    img.get('data-large'),       # –í–µ–ª–∏–∫–∏–π —Ä–æ–∑–º—ñ—Ä
                    img.get('data-src'),         # Lazy loading
                    img.get('data-lazy-src'),    # Lazy loading
                    img.get('src')               # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π
                ]
                
                src = None
                for potential_src in potential_srcs:
                    if potential_src and potential_src.strip():
                        src = potential_src.strip()
                        break
                
                if not src:
                    continue
                    
                # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤—ñ–¥–Ω–æ—Å–Ω—ñ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ñ
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    from urllib.parse import urljoin
                    src = urljoin(article_url, src)
                
                # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –©–ï –ñ–û–†–°–¢–ö–Ü–®–ï
                if any(skip in src.lower() for skip in [
                    'icon', 'logo', 'avatar', 'button', '1x1', 'pixel', 
                    'advertisement', 'banner', 'social', 'share', 'thumb',
                    'widget', 'badge', 'flag', 'arrow', 'spacer', 'clear'
                ]):
                    continue
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä–∏ –ø–æ URL (—è–∫—â–æ –≤–∫–∞–∑–∞–Ω—ñ)
                import re
                size_match = re.search(r'(\d{3,4})x(\d{3,4})', src)
                if size_match:
                    width, height = int(size_match.group(1)), int(size_match.group(2))
                    if width < 800 or height < 600:
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –º–∞–ª–µ–Ω—å–∫—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ —Ä–µ–∞–ª—å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
                if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    # –í–ò–°–û–ö–ò–ô –ü–†–Ü–û–†–ò–¢–ï–¢ –¥–ª—è –≥–æ–ª–æ–≤–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
                    img_classes = ' '.join(img.get('class', [])).lower()
                    img_id = img.get('id', '').lower()
                    parent_classes = ''
                    if img.parent:
                        parent_classes = ' '.join(img.parent.get('class', [])).lower()
                    
                    is_priority = any(pattern in img_classes + img_id + parent_classes for pattern in [
                        'main', 'hero', 'article', 'content', 'featured', 'primary', 
                        'story', 'news', 'photo', 'image', 'big', 'large', 'full'
                    ])
                    
                    if is_priority:
                        priority_images.insert(0, src)  # –ù–∞–π–≤–∏—â–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
                    else:
                        image_urls.append(src)
            
            # –û–±'—î–¥–Ω—É—î–º–æ –∑ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º
            all_images = priority_images + image_urls
            
            logging.info(f"üì∏ –ó–Ω–∞–π–¥–µ–Ω–æ {len(all_images)} –∑–æ–±—Ä–∞–∂–µ–Ω—å –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ ({len(priority_images)} –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–Ω–∏—Ö)")
            return all_images[:15]  # –ë–µ—Ä–µ–º–æ –±—ñ–ª—å—à–µ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–∞—Ç—Ç—é {article_url}: {e}")
            return []

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    import sys
    
    bot = SimpleInstagramBot()
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--test":
            bot.test_run()
        elif sys.argv[1] == "--post":
            bot.create_and_publish_post()

        else:
            print("–î–æ—Å—Ç—É–ø–Ω—ñ –æ–ø—Ü—ñ—ó:")
            print("  --test     : –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø—É—Å–∫")
            print("  --post     : –û–¥–Ω–æ—Ä–∞–∑–æ–≤–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—è")
    else:
        # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º —Ç–µ—Å—Ç
        bot.test_run()

if __name__ == "__main__":
    main()
